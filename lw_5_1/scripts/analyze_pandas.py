# -*- coding: utf-8 -*-
"""analyze_pandas

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MSLZg2pvruowGjRnnbPiHjmTB58aCb86
"""

# -*- coding: utf-8 -*-
"""analyze_pandas

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QeKZ_XO4WTsKi0xoFzvNLhWCwx4SxbWn
"""

#!/usr/bin/env python3
"""
Анализ зарплат в Data Science с использованием Pandas
Задача: найти среднюю зарплату по уровню опыта
"""
import pandas as pd
import sys
import os
import subprocess

def check_hdfs_file_exists(hdfs_path):
    """Проверить существование файла в HDFS"""
    try:
        # Убираем префикс hdfs:// если есть
        if hdfs_path.startswith('hdfs://'):
            hdfs_path = hdfs_path[7:]  # Убираем 'hdfs://'
            # Убираем хост:порт если есть
            if ':' in hdfs_path.split('/')[0]:
                hdfs_path = '/' + '/'.join(hdfs_path.split('/')[1:])

        result = subprocess.run(
            f"hdfs dfs -test -e {hdfs_path}",
            shell=True,
            capture_output=True,
            text=True
        )
        return result.returncode == 0
    except Exception as e:
        print(f"Ошибка при проверке файла в HDFS: {e}")
        return False

def load_data_from_hdfs(hdfs_path):
    """Загрузить данные из HDFS"""
    try:
        # Убираем префикс hdfs:// если есть
        clean_hdfs_path = hdfs_path
        if hdfs_path.startswith('hdfs://'):
            clean_hdfs_path = hdfs_path[7:]  # Убираем 'hdfs://'
            # Убираем хост:порт если есть
            if ':' in clean_hdfs_path.split('/')[0]:
                clean_hdfs_path = '/' + '/'.join(clean_hdfs_path.split('/')[1:])

        print(f"Пытаемся загрузить: {clean_hdfs_path}")

        # Создаем временный локальный файл
        temp_file = "/tmp/salary_data_temp.csv"

        # Копируем файл из HDFS в локальную файловую систему
        copy_command = f"hdfs dfs -get {clean_hdfs_path} {temp_file}"
        print(f"Выполняем команду: {copy_command}")

        result = subprocess.run(copy_command, shell=True, capture_output=True, text=True)

        if result.returncode != 0:
            print(f"Ошибка при копировании из HDFS: {result.stderr}")
            print(f"Stdout: {result.stdout}")
            return None

        # Проверяем, что файл скопировался
        if not os.path.exists(temp_file):
            print(f"Временный файл не создан: {temp_file}")
            return None

        file_size = os.path.getsize(temp_file)
        print(f"Временный файл создан, размер: {file_size} байт")

        # Загружаем данные из временного файла
        df = pd.read_csv(temp_file, low_memory=False)
        print(f"Загружено строк из HDFS: {len(df)}")

        # Удаляем временный файл
        os.remove(temp_file)
        print("Временный файл удален")

        return df

    except Exception as e:
        print(f"Ошибка при загрузке данных из HDFS: {e}")
        # Пытаемся удалить временный файл в случае ошибки
        try:
            if os.path.exists(temp_file):
                os.remove(temp_file)
        except:
            pass
        return None

def load_data(filepath):
    """Загрузить данные из CSV файла (HDFS или локальный)"""
    try:
        # Если путь выглядит как HDFS путь, загружаем из HDFS
        if filepath.startswith('hdfs://') or filepath.startswith('/user/hadoop/'):
            print(f"Пытаемся загрузить из HDFS: {filepath}")
            return load_data_from_hdfs(filepath)
        else:
            # Локальный файл
            if os.path.exists(filepath):
                df = pd.read_csv(filepath, low_memory=False)
                print(f"Загружено строк из локального файла: {len(df)}")
                return df
            else:
                print(f"Локальный файл не найден: {filepath}")
                return None
    except Exception as e:
        print(f"Ошибка при загрузке данных: {e}")
        return None

def clean_data(df):
    """Очистка и подготовка данных"""
    print("\n=== Очистка данных ===")
    print(f"Исходное количество строк: {len(df)}")

    # Удалить строки без зарплаты
    df = df[df['salary_in_usd'].notna()]

    # Заполнить пустые значения в experience_level
    df['experience_level'] = df['experience_level'].fillna('Unknown')

    # Расшифровка кодов уровней опыта
    experience_map = {
        'EN': 'Entry-level',
        'MI': 'Mid-level',
        'SE': 'Senior-level',
        'EX': 'Executive-level',
        'Unknown': 'Unknown'
    }
    df['experience_level_name'] = df['experience_level'].map(experience_map)

    print(f"Количество строк после очистки: {len(df)}")
    print(f"Уникальных уровней опыта: {df['experience_level'].nunique()}")

    return df

def analyze_salary_by_experience(df):
    """Анализ средней зарплаты по уровням опыта"""
    print("\n=== Анализ средней зарплаты по уровням опыта ===")

    # Группировка по уровню опыта и вычисление статистики по зарплате
    result = df.groupby(['experience_level', 'experience_level_name'])['salary_in_usd'].agg([
        'mean', 'count', 'std', 'min', 'max', 'median'
    ]).reset_index()

    result.columns = [
        'Experience_Code', 'Experience_Level', 'Mean_Salary_USD',
        'Count', 'Std_Deviation', 'Min_Salary', 'Max_Salary', 'Median_Salary'
    ]

    # Сортировка по средней зарплате
    result = result.sort_values('Mean_Salary_USD', ascending=False)

    return result

def find_salary_statistics(df):
    """Найти статистику зарплат по уровням опыта"""
    result = analyze_salary_by_experience(df)

    print("\n=== Результаты анализа зарплат ===")
    print("\nУровни опыта по средней зарплате:")
    print(result.to_string(index=False, float_format='%.2f'))

    max_salary_level = result.iloc[0]
    min_salary_level = result.iloc[-1]

    print(f"\nСамая высокооплачиваемая категория: '{max_salary_level['Experience_Level']}' ({max_salary_level['Experience_Code']})")
    print(f"Средний доход по категории: ${max_salary_level['Mean_Salary_USD']:,.2f} USD")
    print(f"Количество специалистов данной категории: {int(max_salary_level['Count'])}")
    print(f"Диапазон зарплат в данной категории: ${max_salary_level['Min_Salary']:,.2f} - ${max_salary_level['Max_Salary']:,.2f} USD")

    print(f"\nНаименее оплачиваемая категория: '{min_salary_level['Experience_Level']}' ({min_salary_level['Experience_Code']})")
    print(f"Средний доход по категории: ${min_salary_level['Mean_Salary_USD']:,.2f} USD")
    print(f"Количество специалистов данной категории: {int(min_salary_level['Count'])}")
    print(f"Диапазон зарплат в данной категории: ${min_salary_level['Min_Salary']:,.2f} - ${min_salary_level['Max_Salary']:,.2f} USD")

    return result

def additional_analysis(df):
    """Дополнительный анализ данных"""
    print("\n=== Дополнительная статистика ===")

    # Общая статистика по зарплатам
    print(f"\nОбщая статистика зарплат:")
    print(f"Средняя зарплата: ${df['salary_in_usd'].mean():,.2f}")
    print(f"Минимальная зарплата: ${df['salary_in_usd'].min():,.2f}")
    print(f"Максимальная зарплата: ${df['salary_in_usd'].max():,.2f}")

    # Распределение по уровням опыта
    print(f"\nРаспределение по уровням опыта:")
    experience_counts = df['experience_level_name'].value_counts()
    for level, count in experience_counts.items():
        percentage = (count / len(df)) * 100
        print(f"  {level}: {count} специалистов ({percentage:.1f}%)")

def check_hdfs_connection():
    """Проверить подключение к HDFS"""
    try:
        result = subprocess.run("hdfs dfs -ls /", shell=True, capture_output=True, text=True)
        return result.returncode == 0
    except:
        return False

def main():
    # Используем точный путь, который нашли
    data_file = '/user/hadoop/input/salary_data.csv'

    print("=== Анализ зарплат в Data Science ===")
    print(f"Файл: {data_file}")

    # Проверим подключение к HDFS
    if check_hdfs_connection():
        print("Подключение к HDFS: OK")

        # Проверим конкретный файл
        if check_hdfs_file_exists(data_file):
            print(f"Файл найден в HDFS: {data_file}")
        else:
            print(f"Файл НЕ найден в HDFS: {data_file}")
            print("Проверим содержимое директории:")
            subprocess.run("hdfs dfs -ls /user/hadoop/input/", shell=True)
            sys.exit(1)
    else:
        print("Подключение к HDFS: FAILED")
        print("Убедитесь, что Hadoop запущен и HDFS доступен")
        sys.exit(1)

    # Загрузка данных
    df = load_data(data_file)

    if df is None:
        print("Не удалось загрузить данные из HDFS")
        sys.exit(1)

    # Показать базовую информацию
    print("\n=== Информация о данных ===")
    print(f"Колонки: {list(df.columns)}")
    print(f"Общее количество записей: {len(df)}")

    print("\nПервые 5 строк:")
    print(df.head())

    # Очистка данных
    df_clean = clean_data(df)

    # Основной анализ
    result = find_salary_statistics(df_clean)

    # Дополнительный анализ
    additional_analysis(df_clean)

    # Сохранить результаты
    output_file = 'results/salary_by_experience.csv'
    os.makedirs('results', exist_ok=True)
    result.to_csv(output_file, index=False)
    print(f"\nРезультаты сохранены в: {output_file}")

    return result

if __name__ == '__main__':
    main()
