# -*- coding: utf-8 -*-
"""analyze_spark

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tTMDaAgUjjFEobW1XmmEE6tyGnL4r1m4
"""

#!/usr/bin/env python3
"""
Анализ зарплат в Data Science с использованием PySpark
Задача: найти среднюю зарплату по уровню опыта
"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, count, stddev, min as spark_min, max as spark_max
from pyspark.sql.functions import when
import sys
import subprocess
import os

def check_local_file_exists(local_path):
    """Проверить существование файла в локальной файловой системе"""
    return os.path.exists(local_path)

def copy_to_hdfs(local_path, hdfs_path):
    """Скопировать файл из локальной файловой системы в HDFS"""
    try:
        # Создаем директорию в HDFS если не существует
        hdfs_dir = os.path.dirname(hdfs_path)
        subprocess.run(
            f"hdfs dfs -mkdir -p {hdfs_dir}",
            shell=True,
            capture_output=True
        )

        # Копируем файл
        result = subprocess.run(
            f"hdfs dfs -put -f {local_path} {hdfs_path}",
            shell=True,
            capture_output=True,
            text=True
        )
        return result.returncode == 0
    except Exception:
        return False

def create_spark_session():
    """Создать Spark сессию"""
    spark = SparkSession.builder \
        .appName("Salary Analysis") \
        .master("local[*]") \
        .config("spark.driver.memory", "2g") \
        .config("spark.executor.memory", "2g") \
        .getOrCreate()
    return spark

def load_data(spark, filepath):
    """Загрузить данные из локального файла"""
    try:
        # Явно указываем, что это локальный файл с префиксом file://
        local_file_path = f"file://{filepath}"
        df = spark.read.csv(local_file_path, header=True, inferSchema=True)
        return df
    except Exception as e:
        print(f"Ошибка при загрузке данных: {e}")
        return None

def clean_and_prepare(df):
    """Очистка и подготовка данных"""
    print("\n=== Очистка данных ===")
    print(f"Исходное количество строк: {df.count()}")

    # Удалить строки с null в зарплате
    df = df.filter(col('salary_in_usd').isNotNull())

    # Заполнить null в experience_level
    df = df.na.fill('Unknown', subset=['experience_level'])

    # Создать колонку с полными названиями уровней опыта
    df = df.withColumn(
        'experience_level_name',
        when(col('experience_level') == 'EN', 'Entry-level')
        .when(col('experience_level') == 'MI', 'Mid-level')
        .when(col('experience_level') == 'SE', 'Senior-level')
        .when(col('experience_level') == 'EX', 'Executive-level')
        .otherwise('Unknown')
    )

    print(f"Количество строк после очистки: {df.count()}")
    distinct_levels = df.select('experience_level').distinct().count()
    print(f"Уникальных уровней опыта: {distinct_levels}")

    return df

def analyze_salary_by_experience(df):
    """Анализ средней зарплаты по уровням опыта"""
    print("\n=== Анализ средней зарплаты по уровням опыта ===")

    # Группировка и агрегация
    result = df.groupBy('experience_level', 'experience_level_name') \
        .agg(
            avg('salary_in_usd').alias('Mean_Salary_USD'),
            count('*').alias('Count'),
            stddev('salary_in_usd').alias('Std_Deviation'),
            spark_min('salary_in_usd').alias('Min_Salary'),
            spark_max('salary_in_usd').alias('Max_Salary')
        ) \
        .orderBy(col('Mean_Salary_USD').desc())

    return result

def additional_analysis(df):
    """Дополнительный анализ данных"""
    print("\n=== Дополнительная статистика ===")

    # Общая статистика по зарплатам
    salary_stats = df.agg(
        avg('salary_in_usd').alias('avg_salary'),
        spark_min('salary_in_usd').alias('min_salary'),
        spark_max('salary_in_usd').alias('max_salary')
    ).collect()[0]

    print(f"Общая статистика зарплат в USD:")
    print(f"Средняя зарплата: ${salary_stats['avg_salary']:,.2f}")
    print(f"Минимальная зарплата: ${salary_stats['min_salary']:,.2f}")
    print(f"Максимальная зарплата: ${salary_stats['max_salary']:,.2f}")

    # Распределение по уровням опыта
    print(f"\nРаспределение по уровням опыта:")
    experience_counts = df.groupBy('experience_level_name').agg(count('*').alias('count')).collect()
    total_count = df.count()

    for row in experience_counts:
        percentage = (row['count'] / total_count) * 100
        print(f"  {row['experience_level_name']}: {row['count']} записей ({percentage:.1f}%)")

def save_spark_results(result, local_output_path, hdfs_output_dir):
    """Сохранить результаты Spark и скопировать в HDFS"""
    try:
        # Создаем локальную директорию
        os.makedirs("results", exist_ok=True)

        # Сохраняем как единый CSV файл через Pandas
        pandas_df = result.toPandas()
        local_csv_file = f"{local_output_path}.csv"
        pandas_df.to_csv(local_csv_file, index=False)
        print(f"Результаты сохранены локально в: {local_csv_file}")

        # Копируем в HDFS
        hdfs_csv_file = f"{hdfs_output_dir}/salary_by_experience_spark.csv"
        if copy_to_hdfs(local_csv_file, hdfs_csv_file):
            print(f"Результаты также сохранены в HDFS: {hdfs_csv_file}")
            return True
        else:
            print("Не удалось сохранить результаты в HDFS")
            return False

    except Exception as e:
        print(f"Ошибка при сохранении результатов: {e}")
        return False

def main():
    # Локальный путь к данным
    local_path = "/opt/data/salary_data.csv"

    # Создать Spark сессию
    spark = create_spark_session()

    print("=== Анализ зарплат в Data Science с использованием PySpark ===")

    # Проверим файл локально
    if not check_local_file_exists(local_path):
        print(f"Ошибка: Файл не найден локально: {local_path}")
        spark.stop()
        sys.exit(1)

    print(f"Файл найден локально, размер: {os.path.getsize(local_path)} байт")

    # Загрузить данные из локального файла
    df = load_data(spark, local_path)

    if df is None:
        print("Ошибка: Не удалось загрузить данные")
        spark.stop()
        sys.exit(1)

    print(f"Успешно загружено строк: {df.count()}")

    # Очистка данных
    df_clean = clean_and_prepare(df)

    # Анализ
    result = analyze_salary_by_experience(df_clean)

    # Показать результаты
    print("\n=== Результаты анализа зарплат ===")
    print("\nУровни опыта по средней зарплате:")
    result.show(truncate=False)

    # Найти уровень с максимальной и минимальной зарплатой
    result_list = result.collect()
    max_salary_row = result_list[0]
    min_salary_row = result_list[-1]

    print(f"\nСамая высокооплачиваемая категория: '{max_salary_row['experience_level_name']}' ({max_salary_row['experience_level']})")
    print(f"Средний доход по категории: ${max_salary_row['Mean_Salary_USD']:,.2f} USD")
    print(f"Количество специалистов данной категории: {max_salary_row['Count']}")
    print(f"Диапазон зарплат в данной категории: ${max_salary_row['Min_Salary']:,.2f} - ${max_salary_row['Max_Salary']:,.2f} USD")

    print(f"\nНаименее оплачиваемая категория: '{min_salary_row['experience_level_name']}' ({min_salary_row['experience_level']})")
    print(f"Средний доход по категории: ${min_salary_row['Mean_Salary_USD']:,.2f} USD")
    print(f"Количество специалистов данной категории: {min_salary_row['Count']}")
    print(f"Диапазон зарплат в данной категории: ${min_salary_row['Min_Salary']:,.2f} - ${min_salary_row['Max_Salary']:,.2f} USD")

    # Дополнительный анализ
    additional_analysis(df_clean)

    # Сохранить результаты
    local_output_path = "results/salary_by_experience_spark"
    hdfs_output_dir = "/user/hadoop/output"

    save_spark_results(result, local_output_path, hdfs_output_dir)

    spark.stop()

if __name__ == '__main__':
    main()