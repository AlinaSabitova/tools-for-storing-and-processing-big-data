# -*- coding: utf-8 -*-
"""analyze_spark

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tTMDaAgUjjFEobW1XmmEE6tyGnL4r1m4
"""

#!/usr/bin/env python3
"""
Анализ зарплат IT-специалистов с использованием PySpark
Задача: найти среднюю зарплату по уровню опыта
"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, count, stddev, min as spark_min, max as spark_max
from pyspark.sql.functions import when
import sys
import subprocess
import os

def check_hdfs_file_exists(hdfs_path):
    """Проверить существование файла в HDFS"""
    try:
        # Убираем префикс hdfs:// если есть
        clean_path = hdfs_path.replace('hdfs://localhost:9000', '')
        result = subprocess.run(
            f"hdfs dfs -test -e {clean_path}",
            shell=True,
            capture_output=True,
            text=True
        )
        return result.returncode == 0
    except Exception:
        return False

def create_spark_session():
    """Создать Spark сессию"""
    spark = SparkSession.builder \
        .appName("Salary Analysis") \
        .master("local[*]") \
        .config("spark.driver.memory", "2g") \
        .config("spark.executor.memory", "2g") \
        .getOrCreate()
    return spark

def load_data(spark, filepath):
    """Загрузить данные из HDFS"""
    try:
        df = spark.read.csv(filepath, header=True, inferSchema=True)
        return df
    except Exception as e:
        return None

def clean_and_prepare(df):
    """Очистка и подготовка данных"""
    print("\n=== Очистка данных ===")
    print(f"Исходное количество строк: {df.count()}")

    # Удалить строки с null в зарплате
    df = df.filter(col('salary_in_usd').isNotNull())

    # Заполнить null в experience_level
    df = df.na.fill('Unknown', subset=['experience_level'])

    # Создать колонку с полными названиями уровней опыта
    df = df.withColumn(
        'experience_level_name',
        when(col('experience_level') == 'EN', 'Entry-level')
        .when(col('experience_level') == 'MI', 'Mid-level')
        .when(col('experience_level') == 'SE', 'Senior-level')
        .when(col('experience_level') == 'EX', 'Executive-level')
        .otherwise('Unknown')
    )

    print(f"Количество строк после очистки: {df.count()}")
    print(f"Уникальных уровней опыта: {df.select('experience_level').distinct().count()}")

    return df

def analyze_salary_by_experience(df):
    """Анализ средней зарплаты по уровням опыта"""
    print("\n=== Анализ средней зарплаты по уровням опыта ===")

    # Группировка и агрегация
    result = df.groupBy('experience_level', 'experience_level_name') \
        .agg(
            avg('salary_in_usd').alias('Mean_Salary_USD'),
            count('*').alias('Count'),
            stddev('salary_in_usd').alias('Std_Deviation'),
            spark_min('salary_in_usd').alias('Min_Salary'),
            spark_max('salary_in_usd').alias('Max_Salary')
        ) \
        .orderBy(col('Mean_Salary_USD').desc())

    return result

def additional_analysis(df):
    """Дополнительный анализ данных"""
    print("\n=== Дополнительная статистика ===")

    # Общая статистика по зарплатам
    salary_stats = df.agg(
        avg('salary_in_usd').alias('avg_salary'),
        spark_min('salary_in_usd').alias('min_salary'),
        spark_max('salary_in_usd').alias('max_salary')
    ).collect()[0]

    print(f"Общая статистика зарплат в USD:")
    print(f"Средняя зарплата: ${salary_stats['avg_salary']:,.2f}")
    print(f"Минимальная зарплата: ${salary_stats['min_salary']:,.2f}")
    print(f"Максимальная зарплата: ${salary_stats['max_salary']:,.2f}")

    # Распределение по уровням опыта
    print(f"\nРаспределение по уровням опыта:")
    experience_counts = df.groupBy('experience_level_name').agg(count('*').alias('count')).collect()
    total_count = df.count()

    for row in experience_counts:
        percentage = (row['count'] / total_count) * 100
        print(f"  {row['experience_level_name']}: {row['count']} записей ({percentage:.1f}%)")

def main():
    # Путь к данным в HDFS
    hdfs_path = "hdfs://localhost:9000/user/hadoop/input/salary_data.csv"

    # Создать Spark сессию
    spark = create_spark_session()

    print("=== Анализ зарплат IT-специалистов с использованием PySpark ===")

    # Проверим файл в HDFS
    if not check_hdfs_file_exists(hdfs_path):
        print(f"Ошибка: Файл не найден в HDFS: {hdfs_path}")
        spark.stop()
        sys.exit(1)

    # Загрузить данные
    df = load_data(spark, hdfs_path)

    if df is None:
        print("Ошибка: Не удалось загрузить данные из HDFS")
        spark.stop()
        sys.exit(1)

    print(f"Успешно загружено строк: {df.count()}")

    # Очистка данных
    df_clean = clean_and_prepare(df)

    # Анализ
    result = analyze_salary_by_experience(df_clean)

    # Показать результаты
    print("\n=== Результаты анализа зарплат ===")
    print("\nУровни опыта по средней зарплате:")
    result.show(truncate=False)

    # Найти уровень с максимальной и минимальной зарплатой
    result_list = result.collect()
    max_salary_row = result_list[0]
    min_salary_row = result_list[-1]

    print(f"\nСамая высокооплачиваемая категория: '{max_salary_row['experience_level_name']}' ({max_salary_row['experience_level']})")
    print(f"Средний доход по категории: ${max_salary_row['Mean_Salary_USD']:,.2f} USD")
    print(f"Количество специалистов данной категории: {max_salary_row['Count']}")
    print(f"Диапазон зарплат в данной категории: ${max_salary_row['Min_Salary']:,.2f} - ${max_salary_row['Max_Salary']:,.2f} USD")

    print(f"\nНаименее оплачиваемая категория: '{min_salary_row['experience_level_name']}' ({min_salary_row['experience_level']})")
    print(f"Средний доход по категории: ${min_salary_row['Mean_Salary_USD']:,.2f} USD")
    print(f"Количество специалистов данной категории: {min_salary_row['Count']}")
    print(f"Диапазон зарплат в данной категории: ${min_salary_row['Min_Salary']:,.2f} - ${min_salary_row['Max_Salary']:,.2f} USD")

    # Дополнительный анализ
    additional_analysis(df_clean)

    # Сохранить результаты локально
    output_path = "results/salary_by_experience_spark"
    os.makedirs("results", exist_ok=True)
    result.coalesce(1).write.mode("overwrite").option("header", "true").csv(output_path)
    print(f"\nРезультаты сохранены в: {output_path}")

    spark.stop()

if __name__ == '__main__':
    main()

#!/usr/bin/env python3
"""
Анализ зарплат в Data Science с использованием PySpark
Задача: найти среднюю зарплату по уровню опыта
"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, count, stddev, min as spark_min, max as spark_max
from pyspark.sql.functions import when
import sys
import subprocess
import os

def check_hdfs_file_exists(hdfs_path):
    """Проверить существование файла в HDFS"""
    try:
        result = subprocess.run(
            f"hdfs dfs -test -e {hdfs_path}",
            shell=True,
            capture_output=True,
            text=True
        )
        return result.returncode == 0
    except Exception:
        return False

def copy_from_hdfs_to_local(hdfs_path, local_path):
    """Скопировать файл из HDFS в локальную файловую систему"""
    try:
        result = subprocess.run(
            f"hdfs dfs -get {hdfs_path} {local_path}",
            shell=True,
            capture_output=True,
            text=True
        )
        return result.returncode == 0
    except Exception:
        return False

def copy_to_hdfs(local_path, hdfs_path):
    """Скопировать файл из локальной файловой системы в HDFS"""
    try:
        # Создаем директорию в HDFS если не существует
        hdfs_dir = os.path.dirname(hdfs_path)
        subprocess.run(
            f"hdfs dfs -mkdir -p {hdfs_dir}",
            shell=True,
            capture_output=True
        )

        # Копируем файл
        result = subprocess.run(
            f"hdfs dfs -put -f {local_path} {hdfs_path}",
            shell=True,
            capture_output=True,
            text=True
        )
        return result.returncode == 0
    except Exception:
        return False

def create_spark_session():
    """Создать Spark сессию"""
    spark = SparkSession.builder \
        .appName("Salary Analysis") \
        .master("local[*]") \
        .config("spark.driver.memory", "2g") \
        .config("spark.executor.memory", "2g") \
        .getOrCreate()
    return spark

def load_data(spark, filepath):
    """Загрузить данные из локального файла"""
    try:
        df = spark.read.csv(filepath, header=True, inferSchema=True)
        return df
    except Exception as e:
        print(f"Ошибка при загрузке данных: {e}")
        return None

def clean_and_prepare(df):
    """Очистка и подготовка данных"""
    print("\n=== Очистка данных ===")
    print(f"Исходное количество строк: {df.count()}")

    # Удалить строки с null в зарплате
    df = df.filter(col('salary_in_usd').isNotNull())

    # Заполнить null в experience_level
    df = df.na.fill('Unknown', subset=['experience_level'])

    # Создать колонку с полными названиями уровней опыта
    df = df.withColumn(
        'experience_level_name',
        when(col('experience_level') == 'EN', 'Entry-level')
        .when(col('experience_level') == 'MI', 'Mid-level')
        .when(col('experience_level') == 'SE', 'Senior-level')
        .when(col('experience_level') == 'EX', 'Executive-level')
        .otherwise('Unknown')
    )

    print(f"Количество строк после очистки: {df.count()}")
    print(f"Уникальных уровней опыта: {df.select('experience_level').distinct().count()}")

    return df

def analyze_salary_by_experience(df):
    """Анализ средней зарплаты по уровням опыта"""
    print("\n=== Анализ средней зарплаты по уровням опыта ===")

    # Группировка и агрегация
    result = df.groupBy('experience_level', 'experience_level_name') \
        .agg(
            avg('salary_in_usd').alias('Mean_Salary_USD'),
            count('*').alias('Count'),
            stddev('salary_in_usd').alias('Std_Deviation'),
            spark_min('salary_in_usd').alias('Min_Salary'),
            spark_max('salary_in_usd').alias('Max_Salary')
        ) \
        .orderBy(col('Mean_Salary_USD').desc())

    return result

def additional_analysis(df):
    """Дополнительный анализ данных"""
    print("\n=== Дополнительная статистика ===")

    # Общая статистика по зарплатам
    salary_stats = df.agg(
        avg('salary_in_usd').alias('avg_salary'),
        spark_min('salary_in_usd').alias('min_salary'),
        spark_max('salary_in_usd').alias('max_salary')
    ).collect()[0]

    print(f"Общая статистика зарплат в USD:")
    print(f"Средняя зарплата: ${salary_stats['avg_salary']:,.2f}")
    print(f"Минимальная зарплата: ${salary_stats['min_salary']:,.2f}")
    print(f"Максимальная зарплата: ${salary_stats['max_salary']:,.2f}")

    # Распределение по уровням опыта
    print(f"\nРаспределение по уровням опыта:")
    experience_counts = df.groupBy('experience_level_name').agg(count('*').alias('count')).collect()
    total_count = df.count()

    for row in experience_counts:
        percentage = (row['count'] / total_count) * 100
        print(f"  {row['experience_level_name']}: {row['count']} записей ({percentage:.1f}%)")

def save_results_to_hdfs(local_results_dir, hdfs_results_dir):
    """Скопировать результаты из локальной директории в HDFS"""
    try:
        # Находим CSV файл в локальной директории
        for file in os.listdir(local_results_dir):
            if file.endswith('.csv'):
                local_file_path = os.path.join(local_results_dir, file)
                hdfs_file_path = f"{hdfs_results_dir}/{file}"

                if copy_to_hdfs(local_file_path, hdfs_file_path):
                    return hdfs_file_path
        return None
    except Exception:
        return None

def main():
    # Путь к данным в HDFS и локальный временный путь
    hdfs_path = "/user/hadoop/input/salary_data.csv"
    local_temp_path = "/tmp/salary_data_spark.csv"

    # Создать Spark сессию
    spark = create_spark_session()

    print("=== Анализ зарплат в Data Science с использованием PySpark ===")

    # Проверим файл в HDFS
    if not check_hdfs_file_exists(hdfs_path):
        print(f"Ошибка: Файл не найден в HDFS: {hdfs_path}")
        spark.stop()
        sys.exit(1)

    # Копируем файл из HDFS в локальную файловую систему
    if not copy_from_hdfs_to_local(hdfs_path, local_temp_path):
        print(f"Ошибка: Не удалось скопировать файл из HDFS: {hdfs_path}")
        spark.stop()
        sys.exit(1)

    # Загрузить данные из локального файла
    df = load_data(spark, local_temp_path)

    if df is None:
        print("Ошибка: Не удалось загрузить данные")
        spark.stop()
        sys.exit(1)

    print(f"Успешно загружено строк: {df.count()}")

    # Удаляем временный файл
    try:
        os.remove(local_temp_path)
    except:
        pass

    # Очистка данных
    df_clean = clean_and_prepare(df)

    # Анализ
    result = analyze_salary_by_experience(df_clean)

    # Показать результаты
    print("\n=== Результаты анализа зарплат ===")
    print("\nУровни опыта по средней зарплате:")
    result.show(truncate=False)

    # Найти уровень с максимальной и минимальной зарплатой
    result_list = result.collect()
    max_salary_row = result_list[0]
    min_salary_row = result_list[-1]

    print(f"\nСамая высокооплачиваемая категория: '{max_salary_row['experience_level_name']}' ({max_salary_row['experience_level']})")
    print(f"Средний доход по категории: ${max_salary_row['Mean_Salary_USD']:,.2f} USD")
    print(f"Количество специалистов данной категории: {max_salary_row['Count']}")
    print(f"Диапазон зарплат в данной категории: ${max_salary_row['Min_Salary']:,.2f} - ${max_salary_row['Max_Salary']:,.2f} USD")

    print(f"\nНаименее оплачиваемая категория: '{min_salary_row['experience_level_name']}' ({min_salary_row['experience_level']})")
    print(f"Средний доход по категории: ${min_salary_row['Mean_Salary_USD']:,.2f} USD")
    print(f"Количество специалистов данной категории: {min_salary_row['Count']}")
    print(f"Диапазон зарплат в данной категории: ${min_salary_row['Min_Salary']:,.2f} - ${min_salary_row['Max_Salary']:,.2f} USD")

    # Дополнительный анализ
    additional_analysis(df_clean)

    # Сохранить результаты локально
    local_output_path = "results/salary_by_experience_spark"
    os.makedirs("results", exist_ok=True)
    result.coalesce(1).write.mode("overwrite").option("header", "true").csv(local_output_path)
    print(f"\nРезультаты сохранены локально в: {local_output_path}")

    # Сохранить результаты в HDFS
    hdfs_output_dir = "/user/hadoop/output"
    hdfs_result_path = save_results_to_hdfs(local_output_path, hdfs_output_dir)

    if hdfs_result_path:
        print(f"Результаты также сохранены в HDFS: {hdfs_result_path}")
    else:
        print("Не удалось сохранить результаты в HDFS")

    spark.stop()

if __name__ == '__main__':
    main()